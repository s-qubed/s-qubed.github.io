<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>WoMAP: World Models For Embodied Open-Vocabulary Object Localization</title>

  <!--  =====  FONTS & ICONS  =====  -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" />
  <link rel="icon" href="./static/icon.png" />

  <!--  =====  CSS  =====  -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick-theme.min.css" />

  <style>
    /* --- GLOBAL --- */
    body {
      background: #fff;
      font-family: "Noto Sans", sans-serif;
      font-size: 16px;
      line-height: 1.5;
      color: #333;
    }

    section {
      padding: 2.5rem 0;
    }

    /* --- SECTION COLORS --- */
    .section-gray {
      background: #f7f7f7;
    }

    /* --- TYPOGRAPHY & LINKS --- */
    .task-title {
      margin-bottom: 1rem;
      font-weight: 600;
    }

    .publication-authors {
      margin-bottom: 1rem;
    }

    /* space below authors */
    .publication-authors a {
      color: #3273dc;
      text-decoration: none;
      white-space: nowrap;
    }

    .publication-authors a:hover {
      text-decoration: underline;
    }

    /* --- HERO & SPACING TWEAKS --- */
    .publication-title {
      margin-top: 0.0rem;
      padding-top: 0.0rem;
    }

    .hero {
      padding-top: 0.75rem;
      padding-bottom: 0.5rem;
    }

    /* tighter gap above overview */
    .hero-body {
      padding: 1.5rem;
    }

    #overview.section {
      padding-top: 0.75rem;
    }

    /* tighter gap below hero */
    .publication-links {
      margin-top: 1.5rem;
      margin-bottom: 1.5rem;
    }

    /* gap between authors/icons & icons/logo */
    figure.lab-logo {
      margin-top: 0.75rem;
    }

    /* gap between icons and Princeton logo */
    /* #sim-robot.section{padding-bottom:0.1rem;} gap above simulation section */
    #bibtex.section- {
      padding-top: 0rem;
    }

    /* gap above BibTeX section */

    /* --- SLICK ARROWS --- */
    .slick-prev:before,
    .slick-next:before {
      color: #3273dc;
      font-size: 32px;
    }

    /* --- THUMBNAILS --- */
    .video-thumb {
      cursor: pointer;
      border-radius: 8px;
      overflow: hidden;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      margin: 0 8px;
    }

    .video-thumb video {
      width: 100%;
      height: auto;
      display: block;
    }

    /* --- STAND‑ALONE VIDEOS --- */
    .overview-video,
    .sim-video {
      width: 100%;
      height: auto;
      border-radius: 0;
      box-shadow: none;
      pointer-events: none;
    }

    /* --- LAYOUT HELPERS --- */
    .task-block {
      margin-bottom: 3rem;
    }

    .task-carousel,
    .task-carousel-tidybot {
      margin-top: 1rem;
    }

    /* --- CONTENT WIDTH CONSISTENCY --- */
    .content-container {
      max-width: 960px;
      margin: 0 auto;
    }

    /* --- PDF EMBED --- */
    .pdf-container {
      margin-top: 1.5rem;
    }

    .pdf-container object {
      width: 100%;
      height: 600px;
      border: none;
    }

    /* --- REAL EXP IMG --- */
    .real-exp-img {
      width: 100%;
      height: auto;
      max-width: 100%;
    }

    /* --- LINK COLOR (lighter blue, closer to default hyperlinks) --- */

    .publication-authors a:hover {
      text-decoration: underline;
    }

    /* --- HERO ↔ OVERVIEW GAP (shrink) --- */
    .hero {
      padding-bottom: 0rem;
    }

    /* was 0.5rem */
    #overview.section {
      padding-top: 0rem;
      padding-bottom: 0rem;
    }

    /* was 0.75rem */

    /* --- REAL-ROBOT ↔ SIMULATION GAP (shrink) --- */
    #real-robot.section {
      padding-top: 2.5rem;
    }

    /* default Bulma ≈2.5rem */
    #real-robot.section {
      padding-bottom: 2.5rem;
    }

    /* default Bulma ≈2.5rem */
    #sim-robot.section {
      padding-top: 2.5rem;
    }

    /* default Bulma ≈2.5rem */

    /* --- SIMULATION ↔ BIBTEX GAP (shrink) --- */
    #BibTeX.section {
      padding-top: 0.1rem;
    }

    /* tighten above BibTeX */
  </style>
</head>

<body>

  <!-- ===== HERO with title & authors ===== -->
  <section class="hero section">
    <div class="hero-body">
      <div class="container is-max-widescreen has-text-centered">
        <h1 class="title is-2 publication-title">WoMAP: World Models For Embodied Open-Vocabulary Object Localization
        </h1>

        <!-- ===== AUTHORS (three rows, no underscores) ===== -->

        <!-- <div class="is-size-5 publication-authors">
          <div>
            <span class="author-block"><a href="https://tenny-yinyijun.github.io/">Tenny&nbsp;Yin*</a></span>,
            <span class="author-block"><a href="https://may0mei.github.io/">Zhiting&nbsp;Mei</a></span>,
            <span class="author-block"><a href="#">Tao&nbsp;Sun</a></span>,
            <span class="author-block"><a href="https://lihzha.github.io/">Lihan&nbsp;Zha</a></span>
          </div>
          <div>
            <span class="author-block"><a href="#">Miyu&nbsp;Yamane<sup>+</sup></a></span>,
            <span class="author-block"><a
                href="https://www.linkedin.com/in/zhou-emily/">Emily&nbsp;Zhou<sup>+</sup></a></span>,
            <span class="author-block"><a
                href="https://www.linkedin.com/in/jeremy-bao/">Jeremy&nbsp;Bao<sup>+</sup></a></span>,
            <span class="author-block"><a href="#">Ola&nbsp;Shorinwa*</a></span>,
            <span class="author-block"><a
                href="https://irom-lab.princeton.edu/majumdar/">Anirudha&nbsp;Majumdar</a></span>
          </div>
        </div>

        <div>
          <sup>&#42;</sup>Equal Contribution.
          <sup>+</sup>Authors contributed equally.
        </div> -->


        <!-- ===== RESOURCE ICONS ===== -->
        <div class="publication-links">
          <a href="#" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener">
            <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
          </a>
          <a href="#" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener">
            <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
          </a>
          <a href="#" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener">
            <span class="icon"><i class="fab fa-github"></i></span><span>Code (Coming Soon!)</span>
          </a>
          <a href="#" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener">
            <span class="icon"><i class="fab fa-youtube"></i></span><span>Video (Coming Soon!)</span>
          </a>
        </div>

        <!-- ===== LAB LOGO ===== -->
        <!--
        <figure class="image is-inline-block lab-logo">
          <img src="static/irom_princeton.png" alt="IROM Lab logo" style="max-width:500px;">
        </figure>
        -->
      </div>
    </div>
  </section>

  <!-- ===== OVERVIEW ===== -->
  <section id="overview" class="section">
    <div class="container is-max-desktop content-container has-text-centered">
      <video class="overview-video" autoplay loop muted playsinline poster="static/thumbnails/banner.jpg">
        <source src="static/videos/overviews/banner.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
      <p style="margin-top:1rem;">We introduce
        <strong>World&nbsp;Models&nbsp;for&nbsp;Active&nbsp;Perception&nbsp;(WoMAP)</strong>, a recipe for training
        open-vocabulary object localization policies that are grounded in the physical world.
      </p>
    </div>
  </section>

  <!-- ===== ABSTRACT ===== -->
  <section id="abstract" class="section">
    <div class="container is-max-desktop content-container">
      <h2 class="title is-3 has-text-centered">Abstract</h2>
      <p>
        Language-instructed active object localization remains a critical challenge for robots, requiring efficient
        exploration of partially observable environments. However, state-of-the-art approaches either struggle to
        generalize beyond demonstration datasets (e.g., imitation learning methods) or fail to generate physically
        grounded actions (e.g., VLMs). To address these limitations, we introduce
        <strong>World&nbsp;Models&nbsp;for&nbsp;Active&nbsp;Perception&nbsp;(WoMAP)</strong>: a recipe for training
        open-vocabulary object localization policies that: (i) uses a Gaussian Splatting-based real-to-sim-to-real
        pipeline for scalable data generation without the need for expert demonstrations, (ii) distills dense rewards
        signals from open-vocabulary object detectors, and (iii) leverages a latent world model for dynamics and rewards
        prediction to ground high-level action proposals at inference time. Rigorous simulation and hardware experiments
        demonstrate WoMAP's superior performance in a broad range of zero-shot object localization tasks, with more than
        <strong>9x</strong> and <strong>2x</strong> higher success rates compared to VLM and diffusion policy baselines,
        respectively. Further, we show that WoMAP achieves strong generalization and sim-to-real transfer on a TidyBot.
      </p>
      <!-- <br>
      <video id="summary-video" class="shadow" controls preload="metadata" width="100%" poster="static/thumbnails/talk_video.jpg">
        <source src="static/videos/talk_video.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video> -->
    </div>
  </section>

  <!-- ===== DATA COLLECTION ===== -->
  <section id="data-collection" class="section">
    <div class="container is-max-desktop content-container">
      <h2 class="title is-3 has-text-centered">Scalable Data Collection</h2>
      <video class="overview-video" autoplay loop muted playsinline poster="static/thumbnails/gaussian-splat.jpg">
        <source src="static/videos/overviews/gaussian-splat.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
      <br>
      <p>
        We introduce a scalable real-to-sim-to-real data generation pipeline that utilizes only a few real-world videos
        to train Gaussian Splats, render novel observations from sampled camera positions, and annotate each observation
        with a pretrained object detector to offer dense training signals.
      </p>
    </div>
  </section>

  <!-- ===== WORLD MODEL ===== -->
  <section id="world-model-design" class="section">
    <div class="container is-max-desktop content-container">
      <h2 class="title is-3 has-text-centered">World Model Design</h2>
      <video class="overview-video" autoplay loop muted playsinline poster="static/thumbnails/architecture.jpg">
        <source src="static/videos/overviews/architecture.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
      <br>
      <p>
        We propose a world model architecture that combines a latent dynamics model with a reward model. The dynamics
        model predicts the next state and the reward model predicts the reward signal based on the current state and
        action. This architecture allows us to ground high-level action proposals in the physical world.
      </p>
    </div>
  </section>

  <!-- ===== EXPERIMENT RESULTS ===== -->
  <section id="experiment-results" class="section">
    <div class="container is-max-desktop content-container">
      <h2 class="title is-3 has-text-centered">Experiments</h2>
      <p class="has-text-justified">
        We benchmark WoMAP against a VLM-based planner (VLM), diffusion policy (DP), and three world-model-only
        planners: WM-CEM,
        WM-Grad, and WM-HR, in four PyBullet (PB) simulation environments and three Gaussian Splatting environments.
        In all tasks, WoMAP attains higher success rates and efficiency scores compared to all other methods.
        Even when only trained on Gaussian Splat scenes (i.e., without an real-world data), WoMAP achieves strong
        sim-to-real transfer, compared to the VLM baseline (see the paper for more details).
      </p>
      <br>

      <!-- ===== PyBullet Evaluations ===== -->
      <div class="task-block" id="pybullet-1">
        <h3 class="title is-4 task-title">PyBullet Simulation Experiments</h3>
        <div class="task-carousel">
          <!-- Six videos with poster extracted via Python script -->
          <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
              poster="static/thumbnails/sim0.jpg" preload="metadata">
              <source src="static/videos/experiments/sim0.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
              poster="static/thumbnails/sim1.jpg" preload="metadata">
              <source src="static/videos/experiments/sim1.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
              poster="static/thumbnails/sim2.jpg" preload="metadata">
              <source src="static/videos/experiments/sim2.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
              poster="static/thumbnails/sim3.jpg" preload="metadata">
              <source src="static/videos/experiments/sim3.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
              poster="static/thumbnails/sim4.jpg" preload="metadata">
              <source src="static/videos/experiments/sim4.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
              poster="static/thumbnails/sim5.jpg" preload="metadata">
              <source src="static/videos/experiments/sim5.mp4" type="video/mp4" />
            </video></div>
        </div>

        <br>

        <!-- Performance plot -->
        <img src="./static/images/pybullet_results.png" class="real-exp-img" alt="PyBullet Evaluation." />
        <p class="has-text-justified">
          <strong>PyBullet evaluation
            tasks and results:</strong>
          Success rates
          (translucent bars) and efficiency scores (solid bars) in active object
          localization across PyBullet scenes (presented in the order of increasing difficulty) and initial-pose
          conditions: easy (E), medium (M), and hard (H).
        </p>
      </div>


      <!-- ===== Gaussian Splatting Evaluations ===== -->
      <div class="task-block" id="gsplat-1">
        <h3 class="title is-4 task-title">Gaussian Splat Simulation Experiments</h3>
        <div class="task-carousel">
          <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
              poster="static/thumbnails/sim0.jpg" preload="metadata">
              <source src="static/videos/experiments/sim0.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
              poster="static/thumbnails/sim1.jpg" preload="metadata">
              <source src="static/videos/experiments/sim1.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
              poster="static/thumbnails/sim2.jpg" preload="metadata">
              <source src="static/videos/experiments/sim2.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
              poster="static/thumbnails/sim3.jpg" preload="metadata">
              <source src="static/videos/experiments/sim3.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
              poster="static/thumbnails/sim4.jpg" preload="metadata">
              <source src="static/videos/experiments/sim4.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
              poster="static/thumbnails/sim5.jpg" preload="metadata">
              <source src="static/videos/experiments/sim5.mp4" type="video/mp4" />
            </video></div>
        </div>
        <br>

        <!-- Performance plot -->
        <img src="./static/images/gsplat_results.png" class="real-exp-img" alt="PyBullet Evaluation." />
        <p class="has-text-justified">
          <strong>Gaussian Splat evaluation
            tasks and results.</strong>
          WoMAP outperforms all baseline methods in all scenes and
          initial-pose conditions.
        </p>
      </div>

      <!-- <div class="row content-container">
        <div class="col-md-12 text-center">
          <ul class="list-inline">
            <li>
              <img src="static/images/pybullet_results.png" class="real-exp-img" alt="Pybullet experiment results" />
              <img src="static/images/gsplat_results.png" class="real-exp-img" alt="Gsplat experiment results" />
            </li>
          </ul>
        </div>
      </div> -->


      <!-- ===== Real-World Evaluations ===== -->
      <div class="task-block" id="task-2">
        <h3 class="title is-4 task-title">Real-World Experiments</h3>
        <div class="task-carousel-tidybot">
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/random_skeleton.jpg" preload="metadata">
              <source src="static/videos/experiments/evaluations/random_skeleton.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/office_banana.jpg" preload="metadata">
              <source src="static/videos/experiments/evaluations/office_banana.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/random_scissors.jpg" preload="metadata">
              <source src="static/videos/experiments/evaluations/random_scissors.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/kitchen_kettle_2.jpg" preload="metadata">
              <source src="static/videos/experiments/evaluations/kitchen_kettle_2.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/office_keyboard.jpg" preload="metadata">
              <source src="static/videos/experiments/evaluations/office_keyboard.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/kitchen_kettle.jpg" preload="metadata">
              <source src="static/videos/experiments/evaluations/kitchen_kettle_2.mp4" type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/random_keyboard.jpg" preload="metadata">
              <source src="static/videos/experiments/evaluations/random_keyboard.mp4" type="video/mp4" />
            </video></div>
        </div>
      </div>
    </div>

  </section>

  <!-- ===== GENERALIZATION PROPERTIES ===== -->
  <section id="generalization-properties" class="section">
    <div class="container is-max-desktop content-container">
      <h2 class="title is-3 has-text-centered">Zero-Shot Generalization</h2>
      <p class="has-text-justified">We examine the visual and semantic generalization capabilities of WoMAP trained
        only on nominal
        conditions in the GS-Random on 10 scenes with 30 trajectories each.
      </p>
      <br>

      <!-- ===== Visual Generalization ===== -->
      <div class="visual-generalization-block" id="map-1">
        <h3 class="title is-4 task-title">Visual Generalization</h3>
        <p class="has-text-justified">
          Here, we evaluate WoMAP in out-of-distribution
          lighting and background conditions.
          WoMAP generalizes well even under extreme
          lighting conditions.
        </p>

        <div class="task-carousel-tidybot">
          <!-- Six videos with poster extracted via Python script -->
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/generalization_blue_keyboard.jpg" preload="metadata">
              <source src="static/videos/experiments/visual_generalization/generalization_blue_keyboard.mp4"
                type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/generalization_purple_scissors.jpg" preload="metadata">
              <source src="static/videos/experiments/visual_generalization/generalization_purple_scissors.mp4"
                type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/generalization_red_skeleton.jpg" preload="metadata">
              <source src="static/videos/experiments/visual_generalization/generalization_red_skeleton.mp4"
                type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/generalization_flowers_background_glasses.jpg"
              preload="metadata">
              <source
                src="static/videos/experiments/visual_generalization/generalization_flowers_background_glasses.mp4"
                type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/generalization_flowers_background_kettle.jpg"
              preload="metadata">
              <source src="static/videos/experiments/visual_generalization/generalization_flowers_background_kettle.mp4"
                type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/generalization_red_scissors.jpg" preload="metadata">
              <source src="static/videos/experiments/visual_generalization/generalization_red_scissors.mp4"
                type="video/mp4" />
            </video></div>
        </div>
      </div>
      <br>
      <br>

      <!-- ===== Semantic Generalization ===== -->
      <div class="semantic-generalization-block" id="sem-gen-1">
        <h3 class="title is-4 task-title">Semantic Generalization</h3>
        <p class="has-text-justified">
          we evaluate semantic generalization of WoMAP to unseen target objects and task instructions across two axes:
          (i) to unseen language instructions to find target objects that were seen during training and (ii) to unseen
          target objects with unseen language instructions.
          We train WoMAP on only a single instance of a "banana," "scissors," and "mug" and ask WoMAP to either find a
          previously-seen
          object with a novel user instruction (e.g., find a yellow object, when trained on a banana)
          or find an unseen object with an unseen instruction (e.g., find a pair of pliers.)
          WoMAP achieves strong zero-shot semantic generalization, summarized in the following figure.
        </p>
        <br>

        <div class="task-carousel-tidybot">
          <!-- Six videos with poster extracted via Python script -->
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/target_banana_query_yellow_object.jpg"
              preload="metadata">
              <source src="static/videos/experiments/semantic_generalization/target_banana_query_yellow_object.mp4"
                type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/target_pliers_query_pliers.jpg" preload="metadata">
              <source src="static/videos/experiments/semantic_generalization/target_pliers_query_pliers.mp4"
                type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/target_mug_query_coffee.jpg" preload="metadata">
              <source src="static/videos/experiments/semantic_generalization/target_mug_query_coffee.mp4"
                type="video/mp4" />
            </video></div>
          <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;" autoplay
              loop muted playsinline poster="static/thumbnails/target_scissors_query_blades.jpg" preload="metadata">
              <source src="static/videos/experiments/semantic_generalization/target_scissors_query_blades.mp4"
                type="video/mp4" />
            </video></div>
        </div>
        <br>


        <!-- Semantic generalization performance plot -->
        <img src="./static/images/semantic_generalization_banana_scissors_mug.png" alt="Semantic Generalization." />
        <p class="has-text-justified">
          <strong>Generalization plots for unseen queries and objects in the same category:</strong> (left)
          <i>banana</i>,
          (center) <i>scissors</i>, (right) <i>mug</i>. We see a positive correlation in semantic similarity (cosine
          distance) of the objects/queries with the most similar object present in our training objects, and the
          efficiency score suggesting the model's performance.
        </p>
      </div>
    </div>
    </div>

    <!-- </strong></p>
    <br>
    <p><strong>TODO: 4 videos in a 2 by 2 grid, scatter plot for query/object relevance on the right</strong></p>
    </div> -->
  </section>

  <!--
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
      </code></pre>
    </div>
  </section>
  -->
  <br>
  <center class="is-size-10">
    The website design was adapted from <a href="https://nerfies.github.io" class="external-link"><span
        class="dnerf">Nerfies</span></a>.
  </center>
  <br>

  <!-- ===== SCRIPTS ===== -->
  <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.js"></script>
  <script>
    $(function () {
      // Init carousels (only those still using .task-carousel wrappers)
      $('.task-carousel').slick({
        slidesToShow: 4,
        slidesToScroll: 1,
        infinite: false,
        arrows: true,
        dots: false,
        lazyLoad: 'ondemand',
        touchMove: true,
        responsive: [
          { breakpoint: 1024, settings: { slidesToShow: 3 } },
          { breakpoint: 768, settings: { slidesToShow: 2 } },
          { breakpoint: 480, settings: { slidesToShow: 1 } }
        ]
      });

      // Init carousels (only those still using .task-carousel wrappers)
      $('.task-carousel-tidybot').slick({
        slidesToShow: 2,
        slidesToScroll: 1,
        infinite: false,
        arrows: true,
        dots: false,
        lazyLoad: 'ondemand',
        touchMove: true,
        responsive: [
          { breakpoint: 1024, settings: { slidesToShow: 1 } },
          { breakpoint: 768, settings: { slidesToShow: 1 } },
          { breakpoint: 480, settings: { slidesToShow: 1 } }
        ]
      });

      // Horizontal track‑pad scroll → carousel navigation; let vertical scroll bubble up
      $('.task-carousel').on('wheel', function (e) {
        const deltaX = e.originalEvent.deltaX;
        const deltaY = e.originalEvent.deltaY;
        if (Math.abs(deltaX) > Math.abs(deltaY)) {
          e.preventDefault();
          $(this).slick(deltaX < 0 ? 'slickPrev' : 'slickNext');
        }
      });
    });
  </script>
</body>

</html>