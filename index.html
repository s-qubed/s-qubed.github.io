<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WoMAP: World Models For Embodied Open-Vocabulary Object Localization</title>

    <!--  =====  FONTS & ICONS  =====  -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" />
    <link rel="icon" href="./static/icon.png" />

    <!--  =====  CSS  =====  -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick-theme.min.css" />

    <style>
        /* --- GLOBAL --- */
        body {
            background: #fff;
            font-family: "Noto Sans", sans-serif;
            font-size: 16px;
            line-height: 1.5;
            color: #333;
        }

        section {
            padding: 2.5rem 0;
        }

        /* --- SECTION COLORS --- */
        .section-gray {
            background: #f7f7f7;
        }

        /* --- TYPOGRAPHY & LINKS --- */
        .task-title {
            margin-bottom: 1rem;
            font-weight: 600;
        }

        .publication-authors {
            margin-bottom: 1rem;
        }

        /* space below authors */
        .publication-authors a {
            color: #3273dc;
            text-decoration: none;
            white-space: nowrap;
        }

        .publication-authors a:hover {
            text-decoration: underline;
        }

        /* --- HERO & SPACING TWEAKS --- */
        .publication-title {
            margin-top: 0.0rem;
            padding-top: 0.0rem;
        }

        .hero {
            padding-top: 0.75rem;
            padding-bottom: 0.5rem;
        }

        /* tighter gap above overview */
        .hero-body {
            padding: 1.5rem;
        }

        #overview.section {
            padding-top: 0.75rem;
        }

        /* tighter gap below hero */
        .publication-links {
            margin-top: 1.5rem;
            margin-bottom: 1.5rem;
        }

        /* gap between authors/icons & icons/logo */
        figure.lab-logo {
            margin-top: 0.75rem;
        }

        /* gap between icons and Princeton logo */
        /* #sim-robot.section{padding-bottom:0.1rem;} gap above simulation section */
        #bibtex.section- {
            padding-top: 0rem;
        }

        /* gap above BibTeX section */

        /* --- SLICK ARROWS --- */
        .slick-prev:before,
        .slick-next:before {
            color: #3273dc;
            font-size: 32px;
        }

        /* --- THUMBNAILS --- */
        .video-thumb {
            cursor: pointer;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            margin: 0 8px;
        }

        .video-thumb video {
            width: 100%;
            height: auto;
            display: block;
        }

        /* --- STAND‑ALONE VIDEOS --- */
        .overview-video,
        .sim-video {
            width: 100%;
            height: auto;
            border-radius: 0;
            box-shadow: none;
            pointer-events: none;
        }

        /* --- LAYOUT HELPERS --- */
        .task-block {
            margin-bottom: 3rem;
        }

        .task-carousel,
        .task-carousel-exp {
            margin-top: 1rem;
        }

        /* --- CONTENT WIDTH CONSISTENCY --- */
        .content-container {
            max-width: 960px;
            margin: 0 auto;
        }

        /* --- PDF EMBED --- */
        .pdf-container {
            margin-top: 1.5rem;
        }

        .pdf-container object {
            width: 100%;
            height: 600px;
            border: none;
        }

        /* --- REAL EXP IMG --- */
        .real-exp-img {
            width: 100%;
            height: auto;
            max-width: 100%;
        }

        /* --- LINK COLOR (lighter blue, closer to default hyperlinks) --- */

        .publication-authors a:hover {
            text-decoration: underline;
        }

        /* --- HERO ↔ OVERVIEW GAP (shrink) --- */
        .hero {
            padding-bottom: 0rem;
        }

        /* was 0.5rem */
        #overview.section {
            padding-top: 0rem;
            padding-bottom: 0rem;
        }

        /* was 0.75rem */

        /* --- REAL-ROBOT ↔ SIMULATION GAP (shrink) --- */
        #real-robot.section {
            padding-top: 2.5rem;
        }

        /* default Bulma ≈2.5rem */
        #real-robot.section {
            padding-bottom: 2.5rem;
        }

        #sim-to-real-1 {
            margin-bottom: 0;
        }

        /* default Bulma ≈2.5rem */
        #sim-robot.section {
            padding-top: 2.5rem;
        }

        /* default Bulma ≈2.5rem */

        /* --- SIMULATION ↔ BIBTEX GAP (shrink) --- */
        #BibTeX.section {
            padding-top: 0.1rem;
        }

        /* tighten above BibTeX */
    </style>
</head>

<body>

    <!-- ===== HERO with title & authors ===== -->
    <section class="hero section">
        <div class="hero-body">
            <div class="container is-max-widescreen has-text-centered">
                <h1 class="title is-2 publication-title">WoMAP: World Models For Embodied <br /> Open-Vocabulary Object
                    Localization
                </h1>

                <!-- ===== AUTHORS (three rows, no underscores) ===== -->

                <div class="is-size-5 publication-authors">
                    <div>
                        <span class="author-block"><a
                                href="https://tenny-yinyijun.github.io/">Tenny&nbsp;Yin*</a></span>,
                        <span class="author-block"><a href="https://may0mei.github.io/">Zhiting&nbsp;Mei</a></span>,
                        <span class="author-block"><a href="#">Tao&nbsp;Sun</a></span>,
                        <span class="author-block"><a href="https://lihzha.github.io/">Lihan&nbsp;Zha</a></span>,
                        <span class="author-block"><a
                                href="https://www.linkedin.com/in/zhou-emily/">Emily&nbsp;Zhou<sup>+</sup></a></span>,
                    </div>
                    <div>
                        <span class="author-block"><a
                                href="https://www.linkedin.com/in/jeremy-bao/">Jeremy&nbsp;Bao<sup>+</sup></a></span>,
                        <span class="author-block"><a
                                href="https://www.linkedin.com/in/miyu-yamane/">Miyu&nbsp;Yamane<sup>+</sup></a></span>,
                        <span class="author-block"><a href="#">Ola&nbsp;Shorinwa*</a></span>,
                        <span class="author-block"><a
                                href="https://irom-lab.princeton.edu/majumdar/">Anirudha&nbsp;Majumdar</a></span>
                    </div>
                </div>

                <div>
                    <sup>&#42;</sup>Equal Contribution.
                    <sup>+</sup>Authors contributed equally.
                </div>

                <!-- ===== RESOURCE ICONS ===== -->
                <div class="publication-links">
                    <a href="static/paper.pdf" class="external-link button is-normal is-rounded is-dark" target="_blank"
                        rel="noopener">
                        <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                    </a>
                    <a href="https://arxiv.org/abs/2506.01600" class="external-link button is-normal is-rounded is-dark"
                        target="_blank" rel="noopener">
                        <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
                    </a>
                    <a href="https://www.youtube.com/watch?v=i1qSlALio-o"
                        class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener">
                        <span class="icon"><i class="fab fa-youtube"></i></span><span>Video</span>
                    </a>
                    <a href="#" class="external-link button is-normal is-rounded is-dark" target="_blank"
                        rel="noopener">
                        <span class="icon"><i class="fab fa-github"></i></span><span>Code (Coming Soon!)</span>
                    </a>
                </div>

                <!-- ===== LAB LOGO ===== -->

                <figure class="image is-inline-block lab-logo">
                    <img src="static/irom_princeton.png" alt="IROM Lab logo" style="max-width:500px;">
                </figure>

            </div>
        </div>
    </section>

    <!-- ===== OVERVIEW ===== -->
    <section id="overview" class="section">
        <div class="container is-max-desktop content-container has-text-centered">
            <video class="overview-video" autoplay loop muted playsinline poster="static/thumbnails/banner.jpg">
                <source src="static/videos/overviews/banner.mp4" type="video/mp4" />
                Your browser does not support the video tag.
            </video>
            <p style="margin-top:1rem;">We introduce
                <strong>World&nbsp;Models&nbsp;for&nbsp;Active&nbsp;Perception&nbsp;(WoMAP)</strong>, a recipe for
                training
                open-vocabulary object localization policies that are grounded in the physical world.
            </p>
        </div>
    </section>

    <!-- ===== ABSTRACT ===== -->
    <section id="abstract" class="section">
        <div class="container is-max-desktop content-container">
            <h2 class="title is-3 has-text-centered">Abstract</h2>
            <p>
                Language-instructed active object localization remains a critical challenge for robots, requiring
                efficient
                exploration of partially observable environments. However, state-of-the-art approaches either struggle
                to
                generalize beyond demonstration datasets (e.g., imitation learning methods) or fail to generate
                physically
                grounded actions (e.g., VLMs). To address these limitations, we introduce
                <strong>World&nbsp;Models&nbsp;for&nbsp;Active&nbsp;Perception&nbsp;(WoMAP)</strong>: a recipe for
                training
                open-vocabulary object localization policies that: (i) uses a Gaussian Splatting-based
                real-to-sim-to-real
                pipeline for scalable data generation without the need for expert demonstrations, (ii) distills dense
                rewards
                signals from open-vocabulary object detectors, and (iii) leverages a latent world model for dynamics and
                rewards
                prediction to ground high-level action proposals at inference time. Rigorous simulation and hardware
                experiments
                demonstrate WoMAP's superior performance in a broad range of zero-shot object localization tasks, with
                more than
                <strong>9x</strong> and <strong>2x</strong> higher success rates compared to VLM and diffusion policy
                baselines,
                respectively. Further, we show that WoMAP achieves strong generalization and sim-to-real transfer on a
                TidyBot.
            </p>
            <!-- <br>
      <video id="summary-video" class="shadow" controls preload="metadata" width="100%" poster="static/thumbnails/talk_video.jpg">
        <source src="static/videos/talk_video.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video> -->
        </div>
    </section>

    <!-- ===== DATA COLLECTION ===== -->
    <section id="data-collection" class="section">
        <div class="container is-max-desktop content-container">
            <h2 class="title is-3 has-text-centered">Scalable Data Collection</h2>
            <video class="overview-video" autoplay loop muted playsinline poster="static/thumbnails/gaussian-splat.jpg">
                <source src="static/videos/overviews/gaussian-splat.mp4" type="video/mp4" />
                Your browser does not support the video tag.
            </video>
            <br>
            <p>
                We introduce a scalable real-to-sim-to-real data generation pipeline that utilizes only a few real-world
                videos
                to train Gaussian Splats, render novel observations from sampled camera positions, and annotate each
                observation
                with a pretrained object detector to offer dense training signals.
            </p>
        </div>
    </section>

    <!-- ===== WORLD MODEL ===== -->
    <section id="world-model-design" class="section">
        <div class="container is-max-desktop content-container">
            <h2 class="title is-3 has-text-centered">World Model Design</h2>
            <video class="overview-video" autoplay loop muted playsinline poster="static/thumbnails/architecture.jpg">
                <source src="static/videos/overviews/architecture.mp4" type="video/mp4" />
                Your browser does not support the video tag.
            </video>
            <br>
            <p>
                We propose a world model architecture that combines a latent dynamics model with a reward model to
                predict and evaluate expected rewards given actions, thereby grounding high-level action proposals in
                the physical world.
                A central innovation of our pipeline is the use of dense reward distillation from open-vocabulary object
                detectors,
                which allows for data-efficient training without relying on image reconstruction objectives.
            </p>
        </div>
    </section>

    <!-- ===== EXPERIMENT RESULTS ===== -->
    <section id="experiment-results" class="section">
        <div class="container is-max-desktop content-container">
            <h2 class="title is-3 has-text-centered">Experiments</h2>
            <p class="has-text-justified">
                We benchmark WoMAP against a VLM-based planner (VLM), diffusion policy (DP), and three world-model-only
                planners: WM-CEM,
                WM-Grad, and WM-HR, in seven challenging PyBullet (PB) and Gaussian Splatting (GS) environments, where
                the robot must
                efficeintly locate the query object from an arbitrary initial view in an unseen scene configuration.
                In all tasks, WoMAP attains higher success rates and efficiency scores compared to all other methods.
                Even when only trained on Gaussian Splat simulation, WoMAP achieves strong
                sim-to-real transfer compared to baseline (see the paper for more details).
            </p>
            <br>

            <!-- ===== PyBullet Evaluations ===== -->
            <div class="task-block" id="pybullet-1">
                <h3 class="title is-4 task-title">PyBullet Simulation Experiments</h3>
                <div class="task-carousel">
                    <!-- Six videos with poster extracted via Python script -->
                    <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/pb/0-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/pb/0-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/pb/1-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/pb/1-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/pb/2-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/pb/2-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/pb/3-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/pb/3-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/pb/4-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/pb/4-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/pb/5-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/pb/5-annotated.mp4" type="video/mp4" />
                        </video></div>
                </div>
                <br>

                <!-- Performance plot -->
                <img src="./static/images/pybullet_results.png" class="real-exp-img" alt="PyBullet Evaluation." />
                <p class="has-text-justified">
                    *Success rates and efficiency scores are represented by transparent and solid bars, respectively.
                    Results are presented in the order of increasing difficulty and initial-pose
                    conditions: easy (E), medium (M), and hard (H).
                </p>
            </div>


            <!-- ===== Gaussian Splatting Evaluations ===== -->
            <div class="task-block" id="gsplat-1">
                <h3 class="title is-4 task-title">Gaussian Splat Simulation Experiments</h3>
                <div class="task-carousel">
                    <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/gs/0-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/gs/0-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
                            poster="static/thumbnails/sexperiments/gs/1-annotatedim1.jpg" preload="metadata">
                            <source src="static/videos/experiments/gs/1-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/gs/2-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/gs/2-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/gs/3-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/gs/3-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/gs/4-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/gs/4-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/gs/5-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/gs/5-annotated.mp4" type="video/mp4" />
                        </video></div>
                </div>
                <br>

                <!-- Performance plot -->
                <img src="./static/images/gsplat_results.png" class="real-exp-img" alt="PyBullet Evaluation." />
            </div>

            <!-- ===== Real-World Evaluations ===== -->
            <div class="task-block" id="sim-to-real-1">
                <h3 class="title is-4 task-title">Zero-Shot Sim-to-Real Transfer</h3>
                <p>
                    We evaluate WoMAP's sim-to-real transfer ability on 20 hardware trials for each of the 3
                    corresponding real-world tasks on the TidyBot. Despite trained entirely in Gaussan Splat simulation,
                    WoMAP transfers effectively to the real world, maintaining nearly the same success rate and
                    efficiency scores in simulation, with a worst-case performance drop of 23%.
                </p>
                <br>
                <div class="task-carousel-exp">
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/evaluations/0-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/evaluations/0-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/evaluations/1-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/evaluations/1-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/evaluations/2-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/evaluations/2-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/evaluations/3-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/evaluations/3-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/evaluations/5-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/evaluations/5-annotated.mp4" type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/evaluations/4-annotated.jpg" preload="metadata">
                            <source src="static/videos/experiments/evaluations/4-annotated.mp4" type="video/mp4" />
                        </video></div>
                </div>
            </div>
        </div>
    </section>

    <!-- ===== GENERALIZATION PROPERTIES ===== -->
    <section id="generalization-properties" class="section">
        <div class="container is-max-desktop content-container">
            <h2 class="title is-3 has-text-centered">Zero-Shot Generalization</h2>
            <!-- ===== Visual Generalization ===== -->
            <div class="visual-generalization-block" id="vis-gen-1">
                <h3 class="title is-4 task-title">Visual Generalization</h3>
                <p class="has-text-justified">
                    We evaluate WoMAP in out-of-distribution lighting and background conditions with the model trained
                    on nominal conditions in simulation, demonstrating the robustness of our dynamics and rewards
                    prediction module, and the rich visual-semantic information encoded in the latent state.
                </p>
                <br>
                <div class="task-carousel-exp">
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/visual_generalization/0-annotated.jpg"
                            preload="metadata">
                            <source src="static/videos/experiments/visual_generalization/0-annotated.mp4"
                                type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/visual_generalization/1-annotated.jpg"
                            preload="metadata">
                            <source src="static/videos/experiments/visual_generalization/1-annotated.mp4"
                                type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/visual_generalization/2-annotated.jpg"
                            preload="metadata">
                            <source src="static/videos/experiments/visual_generalization/2-annotated.mp4"
                                type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/visual_generalization/3-annotated.jpg"
                            preload="metadata">
                            <source src="static/videos/experiments/visual_generalization/3-annotated.mp4"
                                type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/visual_generalization/4-annotated.jpg"
                            preload="metadata">
                            <source src="static/videos/experiments/visual_generalization/4-annotated.mp4"
                                type="video/mp4" />
                        </video></div>
                </div>
            </div>
            <br>
            <br>

            <!-- ===== Semantic Generalization ===== -->
            <div class="semantic-generalization-block" id="sem-gen-1">
                <h3 class="title is-4 task-title">Semantic Generalization</h3>
                <p class="has-text-justified">
                    We evaluate WoMAP to unseen target objects and task instructions across two axes:
                    (i) unseen language instructions for finding target objects that were seen during training, and (ii)
                    unseen
                    target objects with unseen language instructions.
                    WoMAP achieves strong zero-shot semantic generalization, and we observe a positive correlation in
                    semantic similarity (cosine
                    distance) of the objects/queries with the most similar object present in our training objects.
                </p>
                <br>

                <div class="task-carousel-exp">
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/semantic_generalization/0-annotated.jpg"
                            preload="metadata">
                            <source src="static/videos/experiments/semantic_generalization/0-annotated.mp4"
                                type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/semantic_generalization/1-annotated.jpg"
                            preload="metadata">
                            <source src="static/videos/experiments/semantic_generalization/1-annotated.mp4"
                                type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/semantic_generalization/2-annotated.jpg"
                            preload="metadata">
                            <source src="static/videos/experiments/semantic_generalization/2-annotated.mp4"
                                type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/semantic_generalization/3-annotated.jpg"
                            preload="metadata">
                            <source src="static/videos/experiments/semantic_generalization/3-annotated.mp4"
                                type="video/mp4" />
                        </video></div>
                    <div class="video-thumb"><video class="thumb-video" controls onloadstart="this.playbackRate = 1.0;"
                            autoplay loop muted playsinline
                            poster="static/thumbnails/experiments/semantic_generalization/4-annotated.jpg"
                            preload="metadata">
                            <source src="static/videos/experiments/semantic_generalization/4-annotated.mp4"
                                type="video/mp4" />
                        </video></div>
                </div>
                <br>
                <!-- Semantic generalization performance plot -->
                <img src="./static/images/semantic_generalization_banana_scissors_mug.jpg"
                    alt="Semantic Generalization." />
            </div>
        </div>
        </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@misc{yin2025womapworldmodelsembodied,
                title={WoMAP: World Models For Embodied Open-Vocabulary Object Localization}, 
                author={Tenny Yin and Zhiting Mei and Tao Sun and Lihan Zha and Emily Zhou and Jeremy Bao and Miyu Yamane and Ola Shorinwa and Anirudha Majumdar},
                year={2025},
                eprint={2506.01600},
                archivePrefix={arXiv},
                primaryClass={cs.RO},
                url={https://arxiv.org/abs/2506.01600}, 
            }
            </code></pre>
        </div>
    </section>


    <br>
    <center class="is-size-10">
        The website design was adapted from <a href="https://nerfies.github.io" class="external-link"><span
                class="dnerf">Nerfies</span></a>.
    </center>
    <br>

    <!-- ===== SCRIPTS ===== -->
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.js"></script>
    <script>
        $(function () {
            // Init carousels (only those still using .task-carousel wrappers)
            $('.task-carousel').slick({
                slidesToShow: 4,
                slidesToScroll: 1,
                infinite: false,
                arrows: true,
                dots: false,
                lazyLoad: 'ondemand',
                touchMove: true,
                responsive: [
                    { breakpoint: 1024, settings: { slidesToShow: 3 } },
                    { breakpoint: 768, settings: { slidesToShow: 2 } },
                    { breakpoint: 480, settings: { slidesToShow: 1 } }
                ]
            });

            // Horizontal track‑pad scroll → carousel navigation; let vertical scroll bubble up
            $('.task-carousel').on('wheel', function (e) {
                const deltaX = e.originalEvent.deltaX;
                const deltaY = e.originalEvent.deltaY;
                if (Math.abs(deltaX) > Math.abs(deltaY)) {
                    e.preventDefault();
                    $(this).slick(deltaX < 0 ? 'slickPrev' : 'slickNext');
                }
            });

            $('.task-carousel-exp').slick({
                slidesToShow: 3,
                slidesToScroll: 1,
                infinite: false,
                arrows: true,
                dots: false,
                lazyLoad: 'ondemand',
                touchMove: true,
                responsive: [
                    { breakpoint: 1024, settings: { slidesToShow: 1 } },
                    { breakpoint: 768, settings: { slidesToShow: 1 } },
                    { breakpoint: 480, settings: { slidesToShow: 1 } }
                ]
            });

            // Horizontal track‑pad scroll → carousel navigation; let vertical scroll bubble up
            $('.task-carousel').on('wheel', function (e) {
                const deltaX = e.originalEvent.deltaX;
                const deltaY = e.originalEvent.deltaY;
                if (Math.abs(deltaX) > Math.abs(deltaY)) {
                    e.preventDefault();
                    $(this).slick(deltaX < 0 ? 'slickPrev' : 'slickNext');
                }
            });
            $('.task-carousel-exp').on('wheel', function (e) {
                const deltaX = e.originalEvent.deltaX;
                const deltaY = e.originalEvent.deltaY;
                if (Math.abs(deltaX) > Math.abs(deltaY)) {
                    e.preventDefault();
                    $(this).slick(deltaX < 0 ? 'slickPrev' : 'slickNext');
                }
            });

        });
    </script>
</body>

</html>